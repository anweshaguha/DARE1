---
title: "DARE 1"
author: "Anwesha Guha & Merly Klaas"
date: "1/11/2022"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(pacman)

# These are the packages you will need for the analyses 
p_load(here, rio, tidyverse, dplyr, stargazer, fixest, DT, modelsummary, haven , gtsummary, naniar, finalfit)

dare1 <- import(here("data", "EDLD_650_DARE_1.csv"))
```

## A. Data Management Tasks (1 point)

For these tasks, no write up is required. The code you submit will be sufficient.

**A1.** Convert the raw counts of enrollment by race/ethnicity into percentages (i.e., divide the enrollment count for each ethno-racial category by total enrollment). For programming efficiency, can you use a function to do this task?

```{r}
dare1 <- dare1 %>%
  mutate(across(c(10:15), ~ . / !! dare1$enroll * 100))
```

**A2.** Generate dichotomous policy predictor variables that take the value of 1 in state-year observations in which the policy is in place. Call them eval, class remove and suspension. They should take the value of 0 in years during which these policies were not in place. 

```{r}
dare1 <- dare1 %>% 
  mutate(eval = case_when(eval_year>=school_year ~ 1,
         TRUE ~ 0)) %>% 
  mutate(class_remove = case_when(class_remove_year>=school_year ~ 1,
         TRUE ~ 0)) %>% 
  mutate(suspension = case_when(suspension_year>=school_year ~ 1,
         TRUE ~ 0))

#         mutate(eval = ifelse(is.na(eval_year),0,1)) %>% 
#         mutate(class_remove = ifelse(is.na(class_remove_year),0,1)) %>% 
#         mutate(suspension = ifelse(is.na(suspension_year),0,1)) %>% 
#         runtime_classremove = eval_year - class_remove_year,
#         runtime_suspension = eval_year - suspension_year,
#         evalXclass_removeyear = eval * runtime_classremove,
#         evalXsuspyear = eval * runtime_suspension)


```

Also, generate a running time variable (run time) that reflects how far or close the state-year observation is from the implementation of higher stakes teacher evaluation and a variable that permits the effects of the evaluation policy to vary (linearly) over time (evalXyear). How will you deal with states that never implement evaluation? Do that too.
```{r}
dare1 <- dare1 %>% 
  mutate(run_time = ifelse(is.na(eval_year), -99, school_year-eval_year)) %>% # -99 for states that never implement evaluation
  mutate(evalXyear = eval*run_time)
```


## B. Understanding the Data and Descriptive Statistics (3 points)

For the following tasks, give your best attempt at completing the analysis and write-up. If you are unable to conduct the programming or analysis, describe what you are attempting to do and what your results would mean.

*Merly*
**B1.** Inspect your data. What sorts of missingness exist within the data file? What sorts of missingness should concern you? Which do not? In this assignment, please restrict your sample to state-years with non-missing outcomes.
```{r}
dare1 %>% 
  drop_na(ODR_class, ODR_objective, ODR_other, ODR_objective) %>% 
missing_plot()
dare1 %>% summary()
## figure out which variables with NAs to remove
```
After excluding row with missing outcomes, there are only 470 observations left. Missing valuesfound in these following variable: Var eval_year = 71, class_remove_year = 374, suspension_year = 259, PBIS  = 129, based on the missingness pattern reflected in the plot above we see that missing data values do not relate to any other data in the dataset and there is no pattern to the actual values of the missing data themselves. Therefore we can conclude that this is Missing Completely at Random (MCAR). If there is specific pattern



*AG*
**B2.** Graphically display the distribution of the outcome data. What do you notice about the distribution of outcomes? Are there any actions, transformations or sensitivity tests you would like to conduct based on this evidence?
```{r}
outcome_data <- dare1 %>% 
  select(ODR_class, ODR_other, ODR_subjective, ODR_objective)
# maybe pivot_longer --> values to "ODR"

outcome_data %>% 
  ggplot(aes(ODR_subjective)) +
  geom_histogram()

outcome_data %>% 
  ggplot(aes(ODR_objective)) +
  geom_histogram()

outcome_data %>% 
  ggplot(aes(ODR_class)) +
  geom_histogram()

outcome_data %>% 
  ggplot(aes(ODR_other)) +
  geom_histogram()
```

Each ODR outcome data is right-skewed.

*Merly*
**B3.** What is the analytic sample from which you will draw your inferences? To what population are you drawing these inferences? For this analytic sample, reproduce Column 1 of Table 1 from Liebowitz, Porter & Bragg (2022) to create a summary of descriptive statistics for the following data elements. All of these statistics (except for state-year and year enrollment) should be weighted by the state-year population:

* Mean state-year enrollment
* Mean year enrollment
* % low-income (FRPL)
* % Am. Indian/Alask. Native
* % Asian/PI
* % Black
* % Hispanic
* % White
* % state-year observations in which PBIS was successfully implemented
* Classroom ODR rate
* Other location ODR rate
* Subjective-Classroom ODR rate
* Objective-Classroom ODR rate

```{r}
stargazer(attitude)
stargazer(dare1, header= FALSE, type = 'latex')
```
```{r mylatextable, results = "asis"}

dare1 %>%
  select(
         `% low-income (FRPL)` = FRPL_percent,
         `% Am. Indian/Alask. Native` = enroll_AM,
         `% Asian/PI` = enroll_ASIAN,
         `% Black` = enroll_BLACK,
         `% Hispanic ` = enroll_HISP,
         `% White` = enroll_WHITE,
         `% Schools by Year Implementing PBIS` = PBIS,
         `Classroom ODR Rate` = ODR_class,
         `Other location ODR Rate` = ODR_other,
         `Subjective-Classroom ODR rate` = ODR_subjective,
         `Objective-Classroom ODR rate` = ODR_objective) %>% drop_na() %>%
tbl_summary(statistic = list(all_continuous() ~ "{mean} ({sd})")) %>%
  modify_footnote(
    all_stat_cols() ~ "Mean (SD) per school"
  ) %>%
  modify_caption("**Table 1. Summary Statistics**") 
```


Describe the characteristics of your sample as you would report these statistics in an academic paper. How are the characteristics of the sample you will be using for this replication exercise different from the sample in Liebowitz, Porter & Bragg (2022)? How, if at all, do you anticipate this will affect your results?

**B4. Optional Extension** Plot the average classroom (ODR class) and classroom-subjective ODRs (ODR subjective) by how close the stateyear observation is to the implementation of the teacher evaluation policy for the states that implemented evaluation reform. (Note: this is similar to Figure 2 in the original paper ). What do you notice about the raw outcome data plotted against the secular trend? Are there any actions, transformations or sensitivity tests you would like to conduct based on this evidence? Why do we stress plotting these raw averages only for states that implemented evaluation reform? How would including these states alter the interpretation of this figure?

## C. Replication and Extension (6 points)

For the following tasks, give your best attempt at completing the analysis and write-up. If you are unable to conduct the programming or analysis, describe what you are attempting to do and what your results would mean.

*AG*
**C1.** Estimate the effects of the introduction of higher-stakes teacher evaluation reforms on Office Disciplinary Referrals. In one of your models, assume that the effects are constant and in another relax this assumption to allow the effects to differ (linearly) over time. Present these difference-in-differences estimates in a table and the associated writeup as you would report these results in an academic paper. Do you notice any important differences in these results and those reported in the original paper? If so, how would you consider addressing them (it is not necessary at this point for you to actually conduct the analysis, just describe approaches you might take)?

For classroom ODRs:
Assume effects are constant
```{r}
library(fixest)
mod_class_constant <- feols(ODR_class ~ eval |
              state_id + school_year, #default clustering on state id
              data = dare1,
              weights = dare1$enroll) 
summary(mod_class_constant)
```


Allow effects to differ over time
```{r}
mod_class_time <- feols(ODR_class ~ evalXyear |
              state_id + school_year,
              data = dare1,
              vcov = ~school_year^state_id,
              weights = dare1$enroll) 
summary(mod_class_time)
```

For subjective ODRs:

Assume effects are constant
```{r}
mod_subj_constant <- feols(ODR_subjective ~ eval |
              state_id + school_year,
              data = dare1,
              weights = dare1$enroll) 
summary(mod_subj_constant)
```

Allow effects to differ over time
```{r}
mod_subj_time <- feols(ODR_subjective ~ evalXyear |
              state_id + school_year,
              data = dare1,
              vcov = ~school_year^state_id,
              weights = dare1$enroll) 
summary(mod_subj_time)
```


Difference can stem from other controls not being accounted for.

*Merly*
**C2.** Liebowitz et al. (2022) conduct a broad set of robustness checks. For this DARE assignment, you will conduct two (2). First test whether the main results you present in Question C1 are robust to the introduction of potentially simultaneous discipline policy reforms. Present the table and associated write-up as you would report these results in an academic paper. Then select an additional robustness check (either from the paper or not) and present evidence on whether your findings are sensitive to this test.
```{r}

```


**C3.** Write a discussion paragraph in which you present the substantive conclusions of your results about the effects of the introduction of higher-stakes teacher evaluation on ODRs.


**C4. Optional Extension** Use an event-study approach to this difference-in-differences research design to estimate the effects of the introduction of higher-stakes teacher evaluation reforms on Office Disciplinary Referrals (ODRs). Present these findings in an event-study graph. Present the figure and associated write-up as you would report these results in an academic paper. Do you notice any important differences in these results and those reported in the original paper? If so, how would you consider addressing them (At this point, it is not necessary for you to actually conduct the analysis. Just describe approaches you
might take.)?

**C5. Optional Extension** Use one (or more) approaches to present the extent to which the successful implementation of Positive Behavioral Intervention and Supports (PBIS) framework moderating the effects of the introduction of higher-stakes teacher evaluation policies. Present these difference-in-differences estimates and associated write-up as you would report these results in an academic paper. Do you notice any important differences in these results and those reported in the original paper? If so, how would you consider addressing them?